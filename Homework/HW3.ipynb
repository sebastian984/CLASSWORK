{"cells":[{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-99d50b7f3459ff5c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1F8sCd41xyq4"},"source":["# Homework #3\n","\n","**See Canvas for the due date**. Complete all of the following problems. Ideally, the theoretical problems should be answered in a Markdown cell directly underneath the question. If you don't know LaTex/Markdown, you may submit separate handwritten solutions to the theoretical problems. Please do not turn in messy work. Computational problems should be completed in this notebook (using the R kernel is preferred). Computational questions may require code, plots, analysis, interpretation, etc. Working in small groups is allowed, but it is important that you make an effort to master the material and hand in your own work. "]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-45727da4cf2e60db","locked":true,"schema_version":3,"solution":false,"task":false},"id":"EI8q0Jzaxyq9"},"source":["## A. Theoretical Problems"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-085ae288bcb0a9e4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"-NGSXzwyxyrA"},"source":["### A.1 Deriving the t-statistic\n","\n","In this question, we'll walk through the proof that the t-test statistic has a t-distribution. More specifically, consider the simple linear regression model $Y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i$, where $\\varepsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)$, $x_i$ are fixed and known, $\\sigma^2$, $\\beta_0$ and $\\beta_1$ are fixed and unknown, and $i=1,...,n$. We estimate $\\beta_0$ and $\\beta_1$ using least squares, and denote the least squares estimators $\\widehat\\beta_0$ and $\\widehat\\beta_1$.\n","\n","Consider testing the hypotheses $H_0: \\beta_1 = 0 \\,\\,\\, vs \\,\\,\\, H_1: \\beta_1 \\ne 0$. In class, we claimed that the appropriate test statistics is\n","\n","\\begin{align*}\n","t = \\frac{\\widehat\\beta_1}{\\widehat{s.e.}(\\widehat\\beta_1)},\n","\\end{align*}\n","\n","where, for simple linear regression, $$\\widehat{s.e.}(\\widehat\\beta_1) = \\sqrt{\\frac{\\widehat{\\sigma}^2}{\\sum^n_{i=1}\\left(x_i - \\bar{x}\\right)^2}} \\,\\,\\,\\, , \\text{ and } \\,\\,\\,\\,\\, \\,\\, \\widehat{\\sigma}^2 = \\frac{RSS}{(n - 2)}.$$\n","\n","Further, we claimed that $t \\sim t\\left(n - 2 \\right)$. That is, the test statistic $t$ has a t-distribution with $n-2$ degrees of freedom. Let's prove it in a few steps!\n","\n","**A.1 (a) [5 points] Recall that $\\displaystyle\\widehat\\beta_1 \\sim N\\left(\\beta_1, \\,\\, \\frac{\\sigma^2}{\\sum^n_{i=1}\\left(x_i - \\bar{x}\\right)^2} \\right)$. Argue that $\\displaystyle Z = \\frac{\\widehat\\beta_1}{\\sqrt{\\frac{\\sigma^2}{\\sum^n_{i=1}\\left(x_i - \\bar{x}\\right)^2}}}$ has a standard normal distribution under $H_0$. (This should be easy!)**"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-d37a879e11c60c27","locked":false,"points":5,"schema_version":3,"solution":true,"task":false},"id":"L4mELSLLxyrC"},"source":["Since $\\displaystyle\\widehat\\beta_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum^n_{i=1}\\left(x_i - \\bar{x}\\right)^2} \\right)$, if we standardize $\\widehat\\beta_1$, i.e., subtract its mean and divide by its standard deviation, we'll get standard normal random variable."]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-d506ee8cb2f3c017","locked":true,"schema_version":3,"solution":false,"task":false},"id":"fXp-wZNRxyrE"},"source":["For shorthand, let $\\mu_{y_i|x_i} = \\beta_0 + \\beta_1x_i$.\n","\n","**A.1 (b) (i). [5 points]  First, show that $\\displaystyle \\frac{\\sum^n_{i=1}\\left(y_i - \\mu_{y_i|x_i}\\right)^2}{\\sigma^2} \\sim \\chi^2(n)$. This should follow straight from the definition of the $\\chi^2(n)$ distribution given in class, along with some easy algebra!**"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-9bc64ae8773ef16f","locked":false,"points":5,"schema_version":3,"solution":true,"task":false},"id":"S0Lcd508xyrF"},"source":["$Y_i \\overset{iid}{\\sim} N(\\mu_{y_i|x_i}, \\, \\sigma^2)$ implies that $\\left(\\frac{y_i - \\mu_{y_i|x_i}}{\\sigma}\\right) \\sim N(0,1)$. So, by definition of $\\chi^2(n)$,\n","\n","$\\displaystyle \\frac{\\sum^n_{i=1}\\left(y_i - \\mu_{y_i|x_i}\\right)^2}{\\sigma^2} = \\sum^n_{i=1}\\left(\\frac{y_i - \\mu_{y_i|x_i}}{\\sigma}\\right)^2 \\sim \\chi^2(n)$, since $\\left(\\frac{y_i - \\mu_{y_i|x_i}}{\\sigma}\\right)$ is standard normal."]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-121df3cd0f923929","locked":true,"schema_version":3,"solution":false,"task":false},"id":"fJkjLuCGxyrH"},"source":["**A.1 (b) (ii). [10 points] Now, show that $\\displaystyle \\frac{\\sum^n_{i=1}\\left(y_i - \\mu_{y_i|x_i}\\right)^2}{\\sigma^2} = \\frac{\\sum^n_{i=1}\\left(y_i - \\widehat{y}_i\\right)^2}{\\sigma^2} + \\frac{\\sum^n_{i=1}\\left(\\widehat{y}_i - \\mu_{y_i|x_i}\\right)^2}{\\sigma^2}$. (HINT: add and subtract $\\widehat{y}_i$ to the lefthand side.)**\n","\n","\n","Note that the middle term $\\frac{\\sum^n_{i=1}\\left(y_i - \\widehat{y}_i\\right)^2}{\\sigma^2}$ can be written as $\\frac{RSS}{\\sigma^2}$, or equivalently, $\\frac{(n-2)\\widehat\\sigma^2}{\\sigma^2}.$"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-2a22741fa79df60f","locked":false,"points":10,"schema_version":3,"solution":true,"task":false},"id":"CcC2-2wxxyrI"},"source":["\\begin{align*}\n","\\displaystyle \\frac{\\sum^n_{i=1}\\left(y_i - \\mu_{y|x}\\right)^2}{\\sigma^2} &= \\frac{\\sum^n_{i=1}\\left(y_i - \\widehat{y}_i +\\widehat{y}_i - \\mu_{y|x}\\right)^2}{\\sigma^2} = \\frac{\\sum^n_{i=1}\\left(y_i - \\widehat{y}_i\\right)^2}{\\sigma^2} + \\frac{\\sum^n_{i=1}\\left(\\widehat{y}_i - \\mu_{y|x}\\right)^2}{\\sigma^2} + 2\\frac{\\sum^n_{i=1}\\left( y_i - \\widehat{y}_i\\right)\\left(\\widehat{y}_i - \\mu_{y|x} \\right)}{\\sigma^2} \\\\\n","&=\\frac{\\sum^n_{i=1}\\left(y_i - \\widehat{y}_i\\right)^2}{\\sigma^2} + \\frac{\\sum^n_{i=1}\\left(\\widehat{y}_i - \\mu_{y|x}\\right)^2}{\\sigma^2},\n","\\end{align*}\n","\n","because $\\displaystyle\\frac{\\sum^n_{i=1}\\left( y_i - \\widehat{y}_i\\right)\\left(\\widehat{y}_i - \\mu_{y|x} \\right)}{\\sigma^2} = \\frac{\\sum^n_{i=1}\\left( y_i - \\widehat{y}_i\\right)\\widehat{y}_i - \\sum^n_{i=1}\\left( y_i - \\widehat{y}_i\\right) \\mu_{y|x}}{\\sigma^2} = 0$"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-0228edde4cb5393b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"dnj6zguaxyrK"},"source":["Ok, so maybe we won't do the *entire* proof. Note, without having to prove it, that $\\displaystyle\\frac{(n-2)\\widehat\\sigma^2}{\\sigma^2} \\sim \\chi^2(n-2)$. This is because\n","\n","$$\\displaystyle \\underbrace{\\frac{\\sum^n_{i=1}\\left(y_i - \\mu_{y|x}\\right)^2}{\\sigma^2}}_{\\chi^2(n)} = \\underbrace{\\frac{(n-2)\\widehat\\sigma^2}{\\sigma^2}}_{\\chi^2(n - 2)} + \\underbrace{\\frac{\\sum^n_{i=1}\\left(\\widehat{y}_i - \\mu_{y|x}\\right)^2}{\\sigma^2}}_{\\chi^2(2)},$$\n","\n","which follows from some facts about the $\\chi^2$ distribution involving moment generating functions. Math Stat students, you should know how to prove this :) \n","\n","With $\\displaystyle\\frac{(n-2)\\widehat\\sigma^2}{\\sigma^2} \\sim \\chi^2(n-2)$ in hand, let's finish the proof that our test statistic for individual t-tests follows a t distribution with $n-2$ degrees of freedom.\n","\n","By definition, a random variable $T$ has a t-distribution with $d$ degrees of freedom if it can be written as \n","\n","$$T = \\frac{Z}{\\sqrt{W\\big/d}},$$\n","\n","where $Z \\sim N(0,1)$ and $W \\sim \\chi^2(d)$. \n","\n","**A.1 (c) [10 points]  Prove that $t = \\frac{\\widehat\\beta_1}{\\widehat{s.e.}(\\widehat\\beta_1)}$ can be written in the form of a t-distributed random variable with $n-2$ degrees of freedom. Specifically, let $Z$ be defined as in problem A.1 (a), and $W = \\frac{(n-2)\\widehat\\sigma^2}{\\sigma^2}$. Show that**\n","\n","\\begin{align*}\n","\\frac{Z}{\\sqrt{W\\big/(n-2)}} = \\frac{\\widehat\\beta_1}{\\widehat{s.e.}(\\widehat\\beta_1)} = t  ,\n","\\end{align*}\n","\n","which, by definition, means that $t \\sim t(n-2)$."]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-3211466339cd9165","locked":false,"points":10,"schema_version":3,"solution":true,"task":false},"id":"IdYCrQ7yxyrN"},"source":["First note that $W\\big/(n-2) = \\frac{\\widehat\\sigma^2}{\\sigma^2}$. Then:\n","\n","\\begin{align*}\n"," \\frac{Z}{\\sqrt{W\\big/(n-2)}} &= \\frac{\\widehat\\beta_1\\bigg/\\sqrt{\\frac{\\sigma^2}{ \\sum^n_{i=1}\\left(x_i - \\bar{x} \\right)^2}}}{\\sqrt{\\frac{\\widehat\\sigma^2}{\\sigma^2}}} = \\frac{\\widehat\\beta_1}{\\sqrt{\\left(\\frac{\\sigma^2}{\\sum^n_{i=1}\\left(x_i - \\bar{x}\\right)^2}\\right) \\left(\\frac{\\widehat\\sigma^2}{\\sigma^2} \\right)} } \\\\\n","&=\\frac{\\widehat\\beta_1}{\\sqrt{\\frac{\\widehat\\sigma^2}{ \\sum^n_{i=1}\\left(x_i - \\bar{x} \\right)^2}} }  = \\frac{\\widehat\\beta_1}{\\widehat{s.e.}(\\widehat\\beta_1)} = t \\sim t(n-2).\n","\\end{align*}\n"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-79b24654b293516f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1nZa7I8MxyrP"},"source":["### Problem A.2 <span style=\"color: #CFB87C;\">(STAT 5010 Students Only)</span> \n","\n","*Over the next few classes, we will be discussing the use of regression--and statistical modeling more generally--for prediction and explanation. The purpose of this problem is to encourage you to gain a deeper understanding of prediction, explanation, and the relationship between these two concepts.*\n","\n","**[10 points] Read “To Explain or to Predict?” by Galit Shmueli and answer at least three of the following questions. Each answer should be typed in a markdown cell in this notebook, and roughly one paragraph (5-10 sentences) in length. The goal of this assignment is to provide the opportunity to think more rigorously about some of the statistical models that we will encounter in this course. What is their purpose? How can we get the most out of our statistical analysis?**\n","\n","1. What is explanatory modeling? Where is explanatory modeling most often used? What does explanatory modeling have to do with causality?\n","\n","2. What is predictive modeling? Where is predictive modeling most often used? What does predictive modeling have to do with causality?\n","\n","3. How are explanation and prediction different? Is it universally recognized that they are different?\n","4. Describe how, according to Shmueli, the statistical modeling procedure might change based on whether the goal of the modeling is explanation or prediction.\n","\n","5. What are some suggestions that Shmueli gives to the statistical community based on his analysis of prediction and explanation? Do you agree with these suggestions?\n","\n","(If you did this as your \"classwork #0\" assignment, then, lucky you! Review and past your answers here.)"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-9b8d5022f54ff7b9","locked":false,"points":10,"schema_version":3,"solution":true,"task":false},"id":"Ba6-cQEoxyrR"},"source":["Answers will vary..."]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-0de9b95fda0a7a9e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"uYgvVoBFxyrR"},"source":["## B. Computational Problems"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-21e69224141689af","locked":true,"schema_version":3,"solution":false,"task":false},"id":"AOuJYKw0xyrS"},"outputs":[],"source":["library(testthat)"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-58aa870d01eecc4a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"iAKz5NMIxyrT"},"source":["## Problem B.1: Comparing Models (20 points)\n","\n","In this exercise, we will fit multiple different models to the same data and determine which of those models we should ultimately use.\n","\n","The data we will be using is the Auto MPG Data Set from the UCI Machine Learning Repository. It contains technical specifications and performance ratings of many different cars. We will focus on the features that impact the overall `mpg` of each car.\n","\n","In the cell below, code is provided for you to load in the data and rename the columns to be more specific."]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-f29a68c29ccdcb5b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"Z3yEKUasxyrU","outputId":"d3870035-d909-4e8b-bd92-a04de306e254"},"outputs":[{"name":"stderr","output_type":"stream","text":["Registered S3 methods overwritten by 'ggplot2':\n","  method         from \n","  [.quosures     rlang\n","  c.quosures     rlang\n","  print.quosures rlang\n"]},{"data":{"text/plain":["      mpg          cylinders      displacement     horsepower        weight    \n"," Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n"," 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n"," Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n"," Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n"," 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n"," Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n","                                                                               \n","     accel         model_year        origin                    car_name  \n"," Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n"," 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n"," Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n"," Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n"," 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n"," Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n","                                                 (Other)           :365  "]},"metadata":{},"output_type":"display_data"}],"source":["library(ggplot2)\n","\n","\n","mpg.data = na.omit(read.csv(url(paste0(\"https://raw.githubusercontent.com/bzaharatos/\",\n","                               \"-Statistical-Modeling-for-Data-Science-Applications/\",\n","                               \"master/Modern%20Regression%20Analysis%20/Datasets/auto-mpg.csv\"))))\n","\n","\n","names(mpg.data) = c(\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \n","                    \"accel\", \"model_year\", \"origin\", \"car_name\")\n","mpg.data$horsepower = as.numeric(mpg.data$horsepower)\n","summary(mpg.data)"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-bf63422d35ef9cd9","locked":true,"schema_version":3,"solution":false,"task":false},"id":"uDBoaChixyrW"},"source":["#### B.1 (a) [6 points] Three Different Models\n","\n","We will fit three different models to this data:\n","\n","1. `mod.1`: Fits `mpg` as the response with `weight` as the predictor.\n","2. `mod.2`: Fits `mpg` as the response with `weight` and `accel` as predictors.\n","3. `mod.3`: Fits `mpg` as the response with `weight`, `accel` and `horsepower` as predictors.\n","\n","Fit these models in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-8923ffaa87e32cb8","locked":false,"schema_version":3,"solution":true,"task":false},"scrolled":false,"id":"FKmIGWZgxyrY","outputId":"ab845f98-b123-47cf-a8e8-4bdbb85b6341"},"outputs":[{"data":{"text/plain":["\n","Call:\n","lm(formula = mpg ~ weight, data = mpg.data)\n","\n","Residuals:\n","     Min       1Q   Median       3Q      Max \n","-11.9736  -2.7556  -0.3358   2.1379  16.5194 \n","\n","Coefficients:\n","             Estimate Std. Error t value Pr(>|t|)    \n","(Intercept) 46.216524   0.798673   57.87   <2e-16 ***\n","weight      -0.007647   0.000258  -29.64   <2e-16 ***\n","---\n","Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n","\n","Residual standard error: 4.333 on 390 degrees of freedom\n","Multiple R-squared:  0.6926,\tAdjusted R-squared:  0.6918 \n","F-statistic: 878.8 on 1 and 390 DF,  p-value: < 2.2e-16\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["\n","Call:\n","lm(formula = mpg ~ weight + accel, data = mpg.data)\n","\n","Residuals:\n","     Min       1Q   Median       3Q      Max \n","-11.1371  -2.7860  -0.3355   2.4192  16.2096 \n","\n","Coefficients:\n","              Estimate Std. Error t value Pr(>|t|)    \n","(Intercept) 41.0953288  1.8680355  21.999  < 2e-16 ***\n","weight      -0.0072931  0.0002809 -25.966  < 2e-16 ***\n","accel        0.2616504  0.0864755   3.026  0.00265 ** \n","---\n","Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n","\n","Residual standard error: 4.288 on 389 degrees of freedom\n","Multiple R-squared:  0.6997,\tAdjusted R-squared:  0.6982 \n","F-statistic: 453.2 on 2 and 389 DF,  p-value: < 2.2e-16\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["\n","Call:\n","lm(formula = mpg ~ weight + accel + horsepower, data = mpg.data)\n","\n","Residuals:\n","    Min      1Q  Median      3Q     Max \n","-11.079  -2.736  -0.331   2.170  16.262 \n","\n","Coefficients:\n","              Estimate Std. Error t value Pr(>|t|)    \n","(Intercept) 45.6782929  2.4085431  18.965  < 2e-16 ***\n","weight      -0.0057894  0.0005776 -10.024  < 2e-16 ***\n","accel       -0.0020657  0.1233378  -0.017  0.98665    \n","horsepower  -0.0474956  0.0159891  -2.970  0.00316 ** \n","---\n","Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n","\n","Residual standard error: 4.246 on 388 degrees of freedom\n","Multiple R-squared:  0.7064,\tAdjusted R-squared:  0.7041 \n","F-statistic: 311.1 on 3 and 388 DF,  p-value: < 2.2e-16\n"]},"metadata":{},"output_type":"display_data"}],"source":["mod.1 = NA\n","mod.2 = NA\n","mod.3 = NA\n","### BEGIN SOLUTION HERE\n","mod.1 = lm(mpg ~ weight, data=mpg.data)\n","summary(mod.1)\n","\n","mod.2 = lm(mpg ~ weight+accel, data=mpg.data)\n","summary(mod.2)\n","\n","mod.3 = lm(mpg ~ weight+accel+horsepower, data=mpg.data)\n","summary(mod.3)\n","### END SOLUTION HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-4ae6d14635767cee","locked":true,"points":6,"schema_version":3,"solution":false,"task":false},"id":"JPNHJI-AxyrZ","outputId":"d0babe0a-9ef0-434c-f1d7-59d0d8a18d54"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1] \"All models are linear models.\"\n"]}],"source":["# Test Cell\n","# Make sure that each model is a linear model\n","if(test_that(\"Testing model types\", \n","             {(expect_is(mod.1, \"lm\"))\n","              (expect_is(mod.2, \"lm\"))\n","              (expect_is(mod.3, \"lm\"))})){\n","    print(\"All models are linear models.\")\n","}else{\n","    print(\"At least one of the models isn't a linear model!\")\n","    print(\"Make sure you're using the lm() function.\")\n","}\n","\n","### BEGIN HIDDEN TESTS\n","\n","test.mod.1 = lm(mpg ~ weight, data=mpg.data)\n","test.mod.2 = lm(mpg ~ weight+accel, data=mpg.data)\n","test.mod.3 = lm(mpg ~ weight+accel+horsepower, data=mpg.data)\n","\n","# Make sure that mod.1 is only modeling `weight` and has correct coefficients\n","test_that(\"Checking mod.1\", {expect_equal(names(mod.1$coef[2]), \"weight\")\n","                             expect_equal(mod.1$coef[1], test.mod.1$coef[1], tol=1e-5)\n","                             expect_equal(mod.1$coef[2], test.mod.1$coef[2], tol=1e-5)})\n","\n","# mod.2 has 'weight' and 'accel' with correct coefficients\n","test_that(\"Checking mod.2\", {expect_equal(names(mod.2$coef), c('(Intercept)', 'weight', 'accel'))\n","                             expect_equal(mod.2$coef[1], test.mod.2$coef[1], tol=1e-5)\n","                             expect_equal(mod.2$coef[2], test.mod.2$coef[2], tol=1e-5)\n","                             expect_equal(mod.2$coef[3], test.mod.2$coef[3], tol=1e-5)})\n","\n","# mod.3 has 'weight', 'accel' and 'horsepower' with correct coefficients\n","test_that(\"Checking mod.3\", {expect_equal(names(mod.3$coef), c('(Intercept)', 'weight', 'accel', 'horsepower'))\n","                             expect_equal(mod.3$coef[1], test.mod.3$coef[1], tol=1e-5)\n","                             expect_equal(mod.3$coef[2], test.mod.3$coef[2], tol=1e-5)\n","                             expect_equal(mod.3$coef[3], test.mod.3$coef[3], tol=1e-5)\n","                             expect_equal(mod.3$coef[4], test.mod.3$coef[4], tol=1e-5)})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-0a66642d5566919e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"biS9L9DCxyrb"},"source":["**B.1 (b) [9 points] Partial F-Tests**\n","\n","Compare the 3 models using pairwise F-tests to determine which of the three we should use moving forward. It may be helpful to write out the null and alternative hypotheses for these tests.\n","\n","Copy your selected model into the `final.model` variable."]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-4e572db4f1d46f31","locked":false,"schema_version":3,"solution":true,"task":false},"id":"BMSJLU_cxyrc","outputId":"b83ef4f5-de11-4987-8662-14d79b475365"},"outputs":[{"data":{"text/html":["<table>\n","<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n","<tbody>\n","\t<tr><td>390        </td><td>7321.234   </td><td>NA         </td><td>      NA   </td><td>     NA    </td><td>         NA</td></tr>\n","\t<tr><td>389        </td><td>7152.893   </td><td> 1         </td><td>168.3407   </td><td>9.15497    </td><td>0.002645259</td></tr>\n","</tbody>\n","</table>\n"],"text/latex":["\\begin{tabular}{r|llllll}\n"," Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n","\\hline\n","\t 390         & 7321.234    & NA          &       NA    &      NA     &          NA\\\\\n","\t 389         & 7152.893    &  1          & 168.3407    & 9.15497     & 0.002645259\\\\\n","\\end{tabular}\n"],"text/markdown":["\n","| Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) |\n","|---|---|---|---|---|---|\n","| 390         | 7321.234    | NA          |       NA    |      NA     |          NA |\n","| 389         | 7152.893    |  1          | 168.3407    | 9.15497     | 0.002645259 |\n","\n"],"text/plain":["  Res.Df RSS      Df Sum of Sq F       Pr(>F)     \n","1 390    7321.234 NA       NA       NA          NA\n","2 389    7152.893  1 168.3407  9.15497 0.002645259"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<table>\n","<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n","<tbody>\n","\t<tr><td>389        </td><td>7152.893   </td><td>NA         </td><td>      NA   </td><td>      NA   </td><td>         NA</td></tr>\n","\t<tr><td>388        </td><td>6993.840   </td><td> 1         </td><td>159.0527   </td><td>8.823826   </td><td>0.003158152</td></tr>\n","</tbody>\n","</table>\n"],"text/latex":["\\begin{tabular}{r|llllll}\n"," Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n","\\hline\n","\t 389         & 7152.893    & NA          &       NA    &       NA    &          NA\\\\\n","\t 388         & 6993.840    &  1          & 159.0527    & 8.823826    & 0.003158152\\\\\n","\\end{tabular}\n"],"text/markdown":["\n","| Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) |\n","|---|---|---|---|---|---|\n","| 389         | 7152.893    | NA          |       NA    |       NA    |          NA |\n","| 388         | 6993.840    |  1          | 159.0527    | 8.823826    | 0.003158152 |\n","\n"],"text/plain":["  Res.Df RSS      Df Sum of Sq F        Pr(>F)     \n","1 389    7152.893 NA       NA        NA          NA\n","2 388    6993.840  1 159.0527  8.823826 0.003158152"]},"metadata":{},"output_type":"display_data"}],"source":["final.model = NA\n","### BEGIN SOLUTION HERE\n","anova(mod.1, mod.2)\n","# We reject the null for the first test, so mod.2 is significantly better than mod.1, so we use mod.2\n","\n","anova(mod.2, mod.3)\n","# We fail to reject the null, so mod.3 is not significantly better than mod.2, so we keep using mod.2\n","\n","final.model = mod.2\n","### END SOLUTION HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-e263c0782f14b31e","locked":true,"points":9,"schema_version":3,"solution":false,"task":false},"id":"swc3WJfDxyre","outputId":"6fe8bfb5-fd55-4772-c71d-dbf9da1cefe8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1] \"You've selected a model! Make sure you're confident in your answer.\"\n"]}],"source":["# Test Cell\n","if(test_that(\"Check final.model class\", {expect_is(final.model, \"lm\")})){\n","    print(\"You've selected a model! Make sure you're confident in your answer.\")\n","}else{\n","    print(\"final.model is not a linear model.\")\n","    print(\"To copy the selected model use `final.model = mod.#`\")\n","}\n","\n","### BEGIN HIDDEN TESTS\n","# The correct answer is mod.2. Check that there are only three parameters (intercept and 2 features).\n","test_that(\"Check final.model is correct\", {expect_equal(length(final.model$coef), 3)})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-29e1b1769967a8bb","locked":true,"schema_version":3,"solution":false,"task":false},"id":"TSuzdW9Fxyrg"},"source":["**B.1 (c) [6 points] Coefficient Confidence Intervals**\n","\n","Using your selected best model, calculate a $95\\%$ confidence interval for the `weight` parameter (you can use the `confint()` function). Save the lower and upper values into `weight.CI.lower` and `weight.CI.upper` respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-b2b78238d899a5a2","locked":false,"schema_version":3,"solution":true,"task":false},"id":"DTio_PI8xyri","outputId":"956362a4-290f-4ba6-bc33-ce298a90fc07"},"outputs":[{"data":{"text/html":["<table>\n","<thead><tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr></thead>\n","<tbody>\n","\t<tr><th scope=row>weight</th><td>-0.007845314</td><td>-0.006740876</td></tr>\n","</tbody>\n","</table>\n"],"text/latex":["\\begin{tabular}{r|ll}\n","  & 2.5 \\% & 97.5 \\%\\\\\n","\\hline\n","\tweight & -0.007845314 & -0.006740876\\\\\n","\\end{tabular}\n"],"text/markdown":["\n","| <!--/--> | 2.5 % | 97.5 % |\n","|---|---|---|\n","| weight | -0.007845314 | -0.006740876 |\n","\n"],"text/plain":["       2.5 %        97.5 %      \n","weight -0.007845314 -0.006740876"]},"metadata":{},"output_type":"display_data"}],"source":["weight.CI.lower = NA\n","weight.CI.upper = NA\n","\n","### BEGIN SOLUTION HERE\n","CI = confint(mod.2, parm=\"weight\", level=0.95)\n","weight.CI.lower = CI[1, 1]\n","weight.CI.upper = CI[1, 2]\n","CI\n","### END SOLUTION HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-25c69640849a19b9","locked":true,"points":6,"schema_version":3,"solution":false,"task":false},"id":"pCT0DQFyxyri"},"outputs":[],"source":["# Test Cell\n","### BEGIN HIDDEN TESTS\n","test.CI = confint(mod.2, parm=\"weight\", level=0.95)\n","test.weight.CI.lower = test.CI[1, 1]\n","test.weight.CI.upper = test.CI[1, 2]\n","\n","test_that(\"CI is correct\", {expect_equal(weight.CI.lower, test.weight.CI.lower, tol=1e-3)\n","                            expect_equal(weight.CI.upper, test.weight.CI.upper, tol=1e-3)})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-645695e3abdf3c2f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7akL0diAxyrj"},"source":["**B.1 (d) [9 points] Model Comparison**\n","\n","So far, we've used the F-test as a way to choose a \"best\" model among the three proposed. Now let's compare the models according to their mean squared errors (MSE). The MSE is defined as\n","\n","\\begin{align*}\n","MSE = \\frac{1}{n}\\sum^n_{i=1}\\left(y_i - \\widehat{y}_i \\right)^2.\n","\\end{align*}\n","\n","Compute the MSE for each of the three models and save their values into their respective `MSE.#` variables.\n","\n","Which of these models has the best MSE? Do these conclusions agree with the model you selected in part **1.b**? Think about why or why not."]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-fef1353077e7b9cb","locked":false,"schema_version":3,"solution":true,"task":false},"id":"tDv31Hj4xyrk","outputId":"ca38fe63-c686-44ca-fac4-a1b90ed2e48f"},"outputs":[{"data":{"text/html":["18.6766165974193"],"text/latex":["18.6766165974193"],"text/markdown":["18.6766165974193"],"text/plain":["[1] 18.67662"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["18.2471761042759"],"text/latex":["18.2471761042759"],"text/markdown":["18.2471761042759"],"text/plain":["[1] 18.24718"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["17.841429544544"],"text/latex":["17.841429544544"],"text/markdown":["17.841429544544"],"text/plain":["[1] 17.84143"]},"metadata":{},"output_type":"display_data"}],"source":["MSE.1 = NA\n","MSE.2 = NA\n","MSE.3 = NA\n","\n","### BEGIN SOLUTION HERE\n","calc_MSE = function(model, n){\n","    MSE = 1/n * sum((resid(model))^2)\n","    return(MSE)\n","}\n","n = length(mpg.data$mpg)\n","MSE.1 = calc_MSE(mod.1, n)\n","MSE.2 = calc_MSE(mod.2, n)\n","MSE.3 = calc_MSE(mod.3, n)\n","\n","MSE.1\n","MSE.2\n","MSE.3\n","### END SOLUTION HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-161e483b7cbb87b0","locked":true,"points":6,"schema_version":3,"solution":false,"task":false},"id":"XjQEX0Ysxyrl"},"outputs":[],"source":["# Test Cell\n","### BEGIN HIDDEN TESTS\n","# Make sure that mod.3 has the least MSE\n","test_that(\"mod.3 has least MSE\", {expect_lt(MSE.3, MSE.1)\n","                                  expect_lt(MSE.3, MSE.2)})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-d9f3f03eb2686ded","locked":false,"points":3,"schema_version":3,"solution":true,"task":false},"id":"iX6azfGGxyrl"},"source":["The third model, i.e., the full model, has the best MSE, because the RSS will always be lower as we add predictors to our model."]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-8dc894f2c5e49b8e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"mgUqN_qGxyrm"},"source":["### Problem B.2\n","\n","In this problem, we'll use a [dataset](https://dasl.datadescription.com/datafile/amazon-books/?_sfm_methods=Multiple+Regression&_sfm_cases=4+59943) about book prices from Amazon. The data consists of data on $n = 325$ books and includes measurements of:\n","\n","- `aprice`: The price listed on Amazon (dollars)\n","\n","\n","- `lprice`: The book's list price (dollars)\n","\n","\n","- `weight`: The book's weight (ounces)\n","\n","\n","- `pages`: The number of pages in the book\n","\n","\n","- `height`: The book's height (inches)\n","\n","\n","- `width`: The book's width (inches)\n","\n","\n","- `thick`: The thickness of the book (inches)\n","\n","\n","- `cover`: Whether the book is a hard cover of paperback.\n","\n","\n","- And other variables...\n","\n","We'll explore a model that will use `lprice`, `pages`, and `width` to predict `aprice`. Here, I use the same cleaning method described in an example in lecture.\n","\n","\n","**B.2 (a) [2 points] Load the dataset and explore it graphically and numerically. Are there relationships between variables?**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-4b8f00760a2333a1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"4viF8G0Zxyrn","outputId":"294442e0-3aba-4ca1-ce7d-8476b070d69a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Registered S3 methods overwritten by 'ggplot2':\n","  method         from \n","  [.quosures     rlang\n","  c.quosures     rlang\n","  print.quosures rlang\n"]},{"data":{"text/html":["<ol class=list-inline>\n","\t<li>'Title'</li>\n","\t<li>'Author'</li>\n","\t<li>'List.Price'</li>\n","\t<li>'Amazon.Price'</li>\n","\t<li>'Hard..Paper'</li>\n","\t<li>'NumPages'</li>\n","\t<li>'Publisher'</li>\n","\t<li>'Pub.year'</li>\n","\t<li>'ISBN.10'</li>\n","\t<li>'Height'</li>\n","\t<li>'Width'</li>\n","\t<li>'Thick'</li>\n","\t<li>'Weight..oz.'</li>\n","</ol>\n"],"text/latex":["\\begin{enumerate*}\n","\\item 'Title'\n","\\item 'Author'\n","\\item 'List.Price'\n","\\item 'Amazon.Price'\n","\\item 'Hard..Paper'\n","\\item 'NumPages'\n","\\item 'Publisher'\n","\\item 'Pub.year'\n","\\item 'ISBN.10'\n","\\item 'Height'\n","\\item 'Width'\n","\\item 'Thick'\n","\\item 'Weight..oz.'\n","\\end{enumerate*}\n"],"text/markdown":["1. 'Title'\n","2. 'Author'\n","3. 'List.Price'\n","4. 'Amazon.Price'\n","5. 'Hard..Paper'\n","6. 'NumPages'\n","7. 'Publisher'\n","8. 'Pub.year'\n","9. 'ISBN.10'\n","10. 'Height'\n","11. 'Width'\n","12. 'Thick'\n","13. 'Weight..oz.'\n","\n","\n"],"text/plain":[" [1] \"Title\"        \"Author\"       \"List.Price\"   \"Amazon.Price\" \"Hard..Paper\" \n"," [6] \"NumPages\"     \"Publisher\"    \"Pub.year\"     \"ISBN.10\"      \"Height\"      \n","[11] \"Width\"        \"Thick\"        \"Weight..oz.\" "]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["     aprice           lprice           pages           width      \n"," Min.   :  0.77   Min.   :  1.50   Min.   : 24.0   Min.   :4.100  \n"," 1st Qu.:  8.60   1st Qu.: 13.95   1st Qu.:208.0   1st Qu.:5.200  \n"," Median : 10.20   Median : 15.00   Median :320.0   Median :5.400  \n"," Mean   : 13.33   Mean   : 18.58   Mean   :335.9   Mean   :5.585  \n"," 3rd Qu.: 13.13   3rd Qu.: 19.95   3rd Qu.:416.0   3rd Qu.:5.900  \n"," Max.   :139.95   Max.   :139.95   Max.   :896.0   Max.   :9.500  \n","                  NA's   :1        NA's   :2       NA's   :5      \n","     weight          height           thick        cover  \n"," Min.   : 1.20   Min.   : 5.100   Min.   :0.1000   H: 89  \n"," 1st Qu.: 7.80   1st Qu.: 7.900   1st Qu.:0.6000   P:236  \n"," Median :11.20   Median : 8.100   Median :0.9000          \n"," Mean   :12.49   Mean   : 8.163   Mean   :0.9077          \n"," 3rd Qu.:16.00   3rd Qu.: 8.500   3rd Qu.:1.1000          \n"," Max.   :35.20   Max.   :12.100   Max.   :2.1000          \n"," NA's   :9       NA's   :4        NA's   :1               "]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<ol class=list-inline>\n","\t<li>324</li>\n","\t<li>8</li>\n","</ol>\n"],"text/latex":["\\begin{enumerate*}\n","\\item 324\n","\\item 8\n","\\end{enumerate*}\n"],"text/markdown":["1. 324\n","2. 8\n","\n","\n"],"text/plain":["[1] 324   8"]},"metadata":{},"output_type":"display_data"}],"source":["library(ggplot2)\n","\n","amazon = read.csv(url(\"https://raw.githubusercontent.com/bzaharatos/STAT-5630/main/amazon.txt\"), sep = \"\\t\")\n","names(amazon)\n","df = data.frame(aprice = amazon$Amazon.Price, lprice = as.numeric(amazon$List.Price),  \n","                pages = amazon$NumPages, width = amazon$Width, weight = amazon$Weight..oz,  \n","                height = amazon$Height, thick = amazon$Thick, cover = amazon$Hard..Paper)\n","\n","summary(df)\n","\n","#imputing\n","df$lprice[which(is.na(df$lprice))] = mean(df$lprice, na.rm = TRUE)\n","df$weight[which(is.na(df$weight))] = mean(df$weight, na.rm = TRUE)\n","df$pages[which(is.na(df$pages))] = mean(df$pages, na.rm = TRUE)\n","df$height[which(is.na(df$height))] = mean(df$height, na.rm = TRUE)\n","df$width[which(is.na(df$width))] = mean(df$width, na.rm = TRUE)\n","df$thick[which(is.na(df$thick))] = mean(df$thick, na.rm = TRUE)\n","df = df[-205,]\n","summary(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-8a505c3e26929b1c","locked":false,"points":1,"schema_version":3,"solution":true,"task":false},"id":"_Gyaue0Sxyro"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-b86d496cff535dc6","locked":false,"points":1,"schema_version":3,"solution":true,"task":false},"id":"ePVlgYmYxyrp"},"source":["..."]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-d329c8b7fc778dbb","locked":true,"schema_version":3,"solution":false,"task":false},"id":"gveyQBevxyrq"},"source":["**B.2 (b) [3 points] Randomly split the dataset into two parts:**\n","\n","1. **80% of the data stored in the variable `train`. This will be the data that we fit/train our model on.**\n","2. **the remaining 20% of the data stored in the variable `test`. We will use this data to test our predictions.**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-1de5e1f9f303dfcb","locked":false,"schema_version":3,"solution":true,"task":false},"id":"YaK2v3n1xyrs","outputId":"ab030aa7-e88a-47d4-d59e-107e5a84d951"},"outputs":[{"data":{"text/html":["<table>\n","<thead><tr><th></th><th scope=col>aprice</th><th scope=col>lprice</th><th scope=col>pages</th><th scope=col>width</th><th scope=col>weight</th><th scope=col>height</th><th scope=col>thick</th><th scope=col>cover</th></tr></thead>\n","<tbody>\n","\t<tr><th scope=row>201</th><td> 6.99</td><td> 6.99</td><td>192  </td><td>4.2  </td><td> 4.0 </td><td>6.7  </td><td>0.6  </td><td>P    </td></tr>\n","\t<tr><th scope=row>66</th><td>11.46</td><td>17.99</td><td>496  </td><td>5.8  </td><td>19.2 </td><td>8.2  </td><td>1.6  </td><td>H    </td></tr>\n","\t<tr><th scope=row>216</th><td> 8.50</td><td>13.95</td><td>352  </td><td>5.5  </td><td>10.6 </td><td>8.4  </td><td>0.9  </td><td>P    </td></tr>\n","\t<tr><th scope=row>27</th><td>12.11</td><td>18.00</td><td>624  </td><td>5.2  </td><td>19.2 </td><td>8.0  </td><td>1.2  </td><td>P    </td></tr>\n","\t<tr><th scope=row>69</th><td>14.99</td><td>26.95</td><td>211  </td><td>5.5  </td><td> 8.0 </td><td>8.3  </td><td>1.0  </td><td>H    </td></tr>\n","\t<tr><th scope=row>269</th><td>10.17</td><td>14.95</td><td>241  </td><td>5.2  </td><td> 5.6 </td><td>8.0  </td><td>0.5  </td><td>P    </td></tr>\n","</tbody>\n","</table>\n"],"text/latex":["\\begin{tabular}{r|llllllll}\n","  & aprice & lprice & pages & width & weight & height & thick & cover\\\\\n","\\hline\n","\t201 &  6.99 &  6.99 & 192   & 4.2   &  4.0  & 6.7   & 0.6   & P    \\\\\n","\t66 & 11.46 & 17.99 & 496   & 5.8   & 19.2  & 8.2   & 1.6   & H    \\\\\n","\t216 &  8.50 & 13.95 & 352   & 5.5   & 10.6  & 8.4   & 0.9   & P    \\\\\n","\t27 & 12.11 & 18.00 & 624   & 5.2   & 19.2  & 8.0   & 1.2   & P    \\\\\n","\t69 & 14.99 & 26.95 & 211   & 5.5   &  8.0  & 8.3   & 1.0   & H    \\\\\n","\t269 & 10.17 & 14.95 & 241   & 5.2   &  5.6  & 8.0   & 0.5   & P    \\\\\n","\\end{tabular}\n"],"text/markdown":["\n","| <!--/--> | aprice | lprice | pages | width | weight | height | thick | cover |\n","|---|---|---|---|---|---|---|---|---|\n","| 201 |  6.99 |  6.99 | 192   | 4.2   |  4.0  | 6.7   | 0.6   | P     |\n","| 66 | 11.46 | 17.99 | 496   | 5.8   | 19.2  | 8.2   | 1.6   | H     |\n","| 216 |  8.50 | 13.95 | 352   | 5.5   | 10.6  | 8.4   | 0.9   | P     |\n","| 27 | 12.11 | 18.00 | 624   | 5.2   | 19.2  | 8.0   | 1.2   | P     |\n","| 69 | 14.99 | 26.95 | 211   | 5.5   |  8.0  | 8.3   | 1.0   | H     |\n","| 269 | 10.17 | 14.95 | 241   | 5.2   |  5.6  | 8.0   | 0.5   | P     |\n","\n"],"text/plain":["    aprice lprice pages width weight height thick cover\n","201  6.99   6.99  192   4.2    4.0   6.7    0.6   P    \n","66  11.46  17.99  496   5.8   19.2   8.2    1.6   H    \n","216  8.50  13.95  352   5.5   10.6   8.4    0.9   P    \n","27  12.11  18.00  624   5.2   19.2   8.0    1.2   P    \n","69  14.99  26.95  211   5.5    8.0   8.3    1.0   H    \n","269 10.17  14.95  241   5.2    5.6   8.0    0.5   P    "]},"metadata":{},"output_type":"display_data"}],"source":["set.seed(1989) #don't change the seed!\n","### BEGIN SOLUTION HERE\n","n = floor(0.8 * nrow(df))\n","index = sample(seq_len(nrow(df)), size = n)\n","\n","train = df[index, ]\n","test = df[-index, ]\n","head(train)\n","### END SOLUTION HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-137a3ab9abf8cf9d","locked":true,"points":3,"schema_version":3,"solution":false,"task":false},"id":"6v-y_w-xxyrs","outputId":"60f30987-3290-4c13-8555-ca9c2f3a0ba9"},"outputs":[{"ename":"ERROR","evalue":"Error in test_that(\"Are the dimensions of the training set correct?\", : could not find function \"test_that\"\n","output_type":"error","traceback":["Error in test_that(\"Are the dimensions of the training set correct?\", : could not find function \"test_that\"\nTraceback:\n"]}],"source":["### BEGIN HIDDEN TESTS \n","d = c(floor(0.8 * nrow(df)), dim(df)[2])\n","test_that(\"Are the dimensions of the training set correct?\", {expect_equal(d, dim(train))})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-58261fb42393d6c9","locked":true,"schema_version":3,"solution":false,"task":false},"id":"QDBJgDH7xyru"},"source":["**B.2 (c) [3 points] Conduct simple linear regression, using the `lm_amazon = lm()` function and your training set, `train`, with `aprice` as the response, and `lprice` as the predictor.**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-533228877cf1e74c","locked":false,"schema_version":3,"solution":true,"task":false},"scrolled":true,"id":"Qk2GHeK4xyru","outputId":"2d5a7220-5bfc-4f9e-92e6-d2f13b3875cb"},"outputs":[{"data":{"text/plain":["\n","Call:\n","lm(formula = aprice ~ lprice, data = train)\n","\n","Residuals:\n","    Min      1Q  Median      3Q     Max \n","-22.287  -1.873  -0.131   2.022  22.251 \n","\n","Coefficients:\n","            Estimate Std. Error t value Pr(>|t|)    \n","(Intercept) -2.88501    0.40174  -7.181 7.44e-12 ***\n","lprice       0.86162    0.01708  50.442  < 2e-16 ***\n","---\n","Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n","\n","Residual standard error: 3.996 on 257 degrees of freedom\n","Multiple R-squared:  0.9083,\tAdjusted R-squared:  0.9079 \n","F-statistic:  2544 on 1 and 257 DF,  p-value: < 2.2e-16\n"]},"metadata":{},"output_type":"display_data"}],"source":["lm_amazon = lm(aprice ~ lprice, data = train)\n","summary(lm_amazon)"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-55a8a5b4af11f8f2","locked":true,"points":3,"schema_version":3,"solution":false,"task":false},"id":"lj0h2dQoxyrw","outputId":"31a3b780-e16c-4680-9223-d99adcc21203"},"outputs":[{"ename":"ERROR","evalue":"Error in test_that(\"Is lm_amazon the correct type?\", {: could not find function \"test_that\"\n","output_type":"error","traceback":["Error in test_that(\"Is lm_amazon the correct type?\", {: could not find function \"test_that\"\nTraceback:\n"]}],"source":["### BEGIN HIDDEN TESTS\n","lm_amazon_solution = lm(aprice ~ lprice, data = train)\n","test_that(\"Is lm_amazon the correct type?\", {expect_is(lm_amazon, \"lm\")})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-e7ed2811acb6b793","locked":true,"schema_version":3,"solution":false,"task":false},"id":"vNb4fPnlxyrw"},"source":["**B.2 (d) [8 points] We learned that, often, the goal of regression is to make predictions on new data. Let's see how well the model does at predicting values that we left out of the training set.**\n","\n","**We can get a sense of how well the model does at predicting by computing the mean squared prediction error (MSPE):**\n","\n","\\begin{align*}\n","MSPE = \\frac{1}{k}\\sum^k_{i=1}\\left(y^\\star_i - \\widehat{y}^\\star_i \\right)^2 = \\frac{1}{k}\\sum^k_{i=1}\\left(y^\\star_i - \\mathbf{x}^\\star_i\\boldsymbol{\\widehat\\beta} \\right)^2\n","\\end{align*}\n","\n","where:\n","\n","- $k$ is the number of rows in the `test` set.\n","- $y^\\star_i$ is the $i^{th}$ response value in the `test` set.\n","- $\\mathbf{x}_i^\\star = \\left(1,x_{i,1}^\\star, x_{i,2}^\\star,...,x_{i,p}^\\star \\right)$ is the $i^{th}$ set of predictors (and a 1 for the intercept multiplication...) in the `test` set. Notice that it's a row vector.\n","- $\\widehat{\\boldsymbol\\beta}$ is the least squares estimate of $\\boldsymbol\\beta$, fit on the *training set*, `train`.\n","\n","**Compute the MSPE for your `test` data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-aadb03541b002e64","locked":false,"points":8,"schema_version":3,"solution":true,"task":false},"id":"-4uEs5YCxyrx","outputId":"0af71d93-8345-4be2-d551-e1932b37c836"},"outputs":[{"data":{"text/html":["12.5033364870952"],"text/latex":["12.5033364870952"],"text/markdown":["12.5033364870952"],"text/plain":["[1] 12.50334"]},"metadata":{},"output_type":"display_data"}],"source":["pred = predict(lm_amazon, test);\n","mseTest = with(test, mean((aprice - pred)^2)); mseTest"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-bf62ff0152f11a84","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8T02F-jdxyrz"},"source":["**B.2 (e) [8 points] The mean squared error (MSE) is similar to the MSPE, but it uses the training data instead of the test data. Compute the MSE on your `train` data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-a5e88b7b9f95711c","locked":false,"points":8,"schema_version":3,"solution":true,"task":false},"id":"VlazNAqnxyrz","outputId":"360ecbf8-0095-408e-9025-76e8dcebcd62"},"outputs":[{"data":{"text/html":["15.846147718757"],"text/latex":["15.846147718757"],"text/markdown":["15.846147718757"],"text/plain":["[1] 15.84615"]},"metadata":{},"output_type":"display_data"}],"source":["mseTrain = mean(lm_amazon$resid^2); mseTrain"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-86fcdebfac0a0dbd","locked":true,"schema_version":3,"solution":false,"task":false},"id":"lFSwRd-gxyr0"},"source":["### Problem B.3\n","\n","Now let's perform multiple linear regression using the dataset from Problem B.2\n","\n","**B.3 (a) [3 points] Perform MLR, using your training set, using `aprice` as the response and all other variables as predictors. Store your `lm()` object in `mlr_amazon`.**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-771c570013964346","locked":false,"schema_version":3,"solution":true,"task":false},"id":"_FY6hRfnxyr0","outputId":"ecb7ff6d-133f-44bf-cfd4-e6883bd79368"},"outputs":[{"data":{"text/plain":["\n","Call:\n","lm(formula = aprice ~ ., data = train)\n","\n","Residuals:\n","     Min       1Q   Median       3Q      Max \n","-21.9307  -1.9325  -0.3505   1.3749  18.3419 \n","\n","Coefficients:\n","              Estimate Std. Error t value Pr(>|t|)    \n","(Intercept) -3.0723736  3.0674382  -1.002  0.31750    \n","lprice       0.8793710  0.0213252  41.236  < 2e-16 ***\n","pages       -0.0002731  0.0029541  -0.092  0.92641    \n","width        0.4211068  0.3594914   1.171  0.24255    \n","weight      -0.0414342  0.0551740  -0.751  0.45337    \n","height      -0.1524099  0.3484786  -0.437  0.66223    \n","thick       -2.2920002  1.2995299  -1.764  0.07900 .  \n","coverP       1.8928019  0.6611078   2.863  0.00455 ** \n","---\n","Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n","\n","Residual standard error: 3.723 on 251 degrees of freedom\n","Multiple R-squared:  0.9222,\tAdjusted R-squared:  0.9201 \n","F-statistic: 425.2 on 7 and 251 DF,  p-value: < 2.2e-16\n"]},"metadata":{},"output_type":"display_data"}],"source":["mlr_amazon = lm(aprice ~ ., data = train)\n","summary(mlr_amazon)"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-f0a0dfdf6b8a7539","locked":true,"points":3,"schema_version":3,"solution":false,"task":false},"id":"yTBuUGPAxyr1"},"outputs":[],"source":["### BEGIN HIDDEN TESTS\n","mlr_amazon_solution = lm(aprice ~ ., data = train)\n","b_solution = coef(mlr_amazon_solution)\n","b = coef(mlr_amazon)\n","test_that(\"Is lm_amazon the correct type?\", {expect_is(lm_amazon, \"lm\")})\n","test_that(\"Is lm_amazon correct?\", {expect_equal(b, b_solution)})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-2e3ba31391c7cb7d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7-xjL1mJxyr1"},"source":["**B.3 (b) [13 points] Compute the MSE for the data in the training set (store in `mse_train`), and for data in the testing set (store in `mse_test`). Which one is lower? Explain why you think it's lower.**"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-2aaf563ccda4f643","locked":false,"schema_version":3,"solution":true,"task":false},"id":"q_weTzXHxyr2","outputId":"312f85dc-ecd6-41c7-ac65-28d07f70f838"},"outputs":[{"data":{"text/html":["15.846147718757"],"text/latex":["15.846147718757"],"text/markdown":["15.846147718757"],"text/plain":["[1] 15.84615"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["12.5033364870952"],"text/latex":["12.5033364870952"],"text/markdown":["12.5033364870952"],"text/plain":["[1] 12.50334"]},"metadata":{},"output_type":"display_data"}],"source":["mse_train = mean(mlr_amazon$resid^2); mseTrain\n","pred = predict(mlr_amazon, test);\n","mse_test = with(test, mean((aprice - pred)^2)); mseTest"]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-dbbe208a1352a563","locked":true,"points":7,"schema_version":3,"solution":false,"task":false},"id":"xOU5O17vxyr3"},"outputs":[],"source":["### BEGIN HIDDEN TESTS\n","mse_train_solution = mean(mlr_amazon_solution$resid^2); \n","pred_solution = predict(mlr_amazon_solution, test);\n","mse_test_solution = with(test, mean((aprice - pred_solution)^2))\n","test_that(\"Is mse_train correct?\", {expect_equal(mse_train, mse_train_solution)})\n","test_that(\"Is lm_amazon correct?\", {expect_equal(mse_test, mse_test_solution)})\n","### END HIDDEN TESTS"]},{"cell_type":"markdown","metadata":{"collapsed":true,"nbgrader":{"grade":true,"grade_id":"cell-b77996c429f388e3","locked":false,"points":6,"schema_version":3,"solution":true,"task":false},"id":"hVeLMohFxyr4"},"source":["The MSE for the test set is larger because the model was not trained on this data. But that's the point! We want to see how well we do predicting the Amazon price for future list prices and dimensions (i.e., list prices and dimensions that weren't used to train the model). "]}],"metadata":{"celltoolbar":"Create Assignment","kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.6.1"},"colab":{"name":"STAT4010-5010_Sp22_HW3_Solutions.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}